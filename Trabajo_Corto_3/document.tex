\documentclass[journal]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{listings}
\usepackage{float}

\lstset{basicstyle=\ttfamily\footnotesize,columns=flexible,breaklines=true}

\title{Trabajo Práctico 3}

\author{
  \IEEEauthorblockN{Kenneth Eduardo Barboza Corrales\IEEEauthorrefmark{1} \and David Alberto Guevara Sánchez\IEEEauthorrefmark{1}}\\
  \IEEEauthorblockA{\IEEEauthorrefmark{1}Maestría en Ciencias de la Computación, Tecnológico de Costa Rica}
}

\begin{document}

\maketitle

\begin{abstract}
	Este trabajo presenta un experimento de agrupamiento no supervisado sobre un conjunto de 33,693 imágenes en escala de grises (48x48 píxeles) que representan emociones humanas. Se aplicó preprocesamiento mediante ecualización de histograma, normalización y reducción de dimensionalidad con PCA (201 componentes, 90\% varianza explicada). Se evaluaron dos algoritmos: K-Means y Gaussian Mixture Model (GMM), utilizando métricas internas Silhouette y Davies-Bouldin. Los resultados muestran una baja separación entre clústeres, con valores óptimos en un rango pequeño de valores de $k$.
\end{abstract}

\begin{IEEEkeywords}
	Aprendizaje no supervisado, Clustering, K-Means, Gaussian Mixture Model, PCA, Silhouette, Davies-Bouldin
\end{IEEEkeywords}

\section{Introducción}

Este trabajo aplica técnicas de agrupamiento no supervisado para descubrir patrones en un conjunto de imágenes faciales en escala de grises. El objetivo es evaluar si los datos presentan una estructura coherente con categorías emocionales, sin disponer de etiquetas previas.

El preprocesamiento incluyó ecualización de histograma, normalización y reducción de dimensionalidad mediante PCA~\cite{jolliffe2002pca}, manteniendo el 90\% de la varianza explicada. Se implementaron dos algoritmos: K-Means~\cite{macqueen1967kmeans} y Gaussian Mixture Models (GMM)~\cite{dempster1977em}.

La calidad del agrupamiento se evaluó en función del número de clústeres ($k$) mediante métricas internas (Silhouette~\cite{rousseeuw1987silhouette} y Davies-Bouldin~\cite{davies1979dbi}) y el criterio del codo.

El conjunto de datos suministrado contiene 35,887 vectores de 2,304 pixeles (48x48, 8 bits) que representan expresiones faciales en escala de grises. En este artículo documentamos el preprocesamiento aplicado, los parámetros elegidos y los resultados cuantitativos de forma breve para que cualquier lector pueda replicar el experimento.

\section{Trabajos relacionados - Estado del arte}

El agrupamiento de imágenes faciales es un problema estudiado en diversos contextos. K-Means~\cite{macqueen1967kmeans} y GMM~\cite{dempster1977em} son métodos comunes de aprendizaje no supervisado para descubrir patrones en datos sin etiquetas.

PCA~\cite{jolliffe2002pca} es una técnica estándar para reducir la dimensionalidad en imágenes de alta dimensionalidad. Reduce el coste computacional mientras preserva la mayor parte de la información relevante.

En reconocimiento facial y análisis de expresiones, el agrupamiento no supervisado enfrenta desafíos debido a la alta variabilidad dentro de cada clase y la similitud entre clases. Métricas como Silhouette~\cite{rousseeuw1987silhouette} y Davies-Bouldin~\cite{davies1979dbi} permiten evaluar calidad de agrupamiento sin etiquetas verdaderas.

\section{Conceptos clave (Background)}

\begin{itemize}
	\item \textbf{Aprendizaje no supervisado:} búsqueda de patrones en datos sin etiquetas de clase.
	\item \textbf{Reducción de dimensionalidad (PCA):} el \textit{Análisis de Componentes Principales (PCA)}~\cite{jolliffe2002pca} transforma los datos originales en un espacio reducido, preservando la mayor varianza posible. En este trabajo se utilizaron 201 componentes (reducción al 8.7\% con respecto al original), reteniendo el 90\% de la varianza.
	\item \textbf{Algoritmo K-Means:}~\cite{macqueen1967kmeans} algoritmo de partición que agrupa datos según proximidad a centroides. Asume clústeres esféricos y de tamaño similar.
	\item \textbf{Modelo de Mezcla Gaussiana (GMM):}~\cite{dempster1977em} modelo probabilístico que asume que los datos provienen de una mezcla de distribuciones gaussianas. Cada punto tiene una probabilidad de pertenecer a cada clúster.
	\item \textbf{Métricas Internas:}
	      \begin{itemize}
			      \item \textit{Silhouette}~\cite{rousseeuw1987silhouette}: mide la cohesión y la separación de los clústeres. Valores en $[-1, 1]$; mayor es mejor.
		      \item \textit{Índice Davies-Bouldin (DBI)}~\cite{davies1979dbi}: evalúa la relación entre dispersión intra-clúster y separación inter-clúster. Menor es mejor.
	      \end{itemize}
\end{itemize}

\section{Materiales y métodos}

\subsection{Conjunto de Datos}

Se utilizó el conjunto de datos proporcionado para el trabajo, compuesto por 35,887 imágenes en escala de grises de 48x48 píxeles (ver Figura~\ref{fig:face33_raw}). Cada fila del archivo CSV almacena 2,304 intensidades de 8 bits, por lo que los ejemplos pueden reconstruirse directamente como matrices $48 \times 48$.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/face33.png}
	\caption{Imagen con el índice $33$ del \textit{dataset} antes del preprocesamiento}
	\label{fig:face33_raw}
\end{figure}

Antes de preprocesar el conjunto se procedió a:

\begin{itemize}
	\item Eliminar 1,853 filas duplicadas.
	\item Descartar 341 imágenes con menor varianza (percentil inferior 1\%). Las Figuras~\ref{fig:worst10} y~\ref{fig:best10} muestran ejemplos de imágenes con muy baja y alta varianza.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/worst10.png}
	\caption{Imágenes con muy poca varianza}
	\label{fig:worst10}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/best10.png}
	\caption{Imágenes con mayor varianza}
	\label{fig:best10}
\end{figure}

Se verificó que no existieran valores faltantes ni píxeles fuera del rango $[0, 255]$. Además, al analizar el histograma de varianzas se observó una cola inferior con rostros prácticamente planos; por eso se descartó el percentil inferior 1\%, que coincide con apenas 341 filas y mejora el contraste de entrada sin sacrificar volumen. Después de estas limpiezas quedaron 33,693 imágenes (93.9\% del conjunto original), número que se usa en todas las corridas posteriores.

\subsection{Preprocesamiento}

\begin{itemize}
	\item \textbf{Ecualización de histograma:} mejora el contraste global de cada rostro, lo que ayuda a resaltar cejas, ojos y boca antes de aplicar PCA (ver Figura~\ref{fig:face33}). Se implementó mediante OpenCV siguiendo la guía clásica de procesamiento digital~\cite{gonzalez2018dip}.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/beforeAfterEqui.png}
	\caption{Comparación de la imagen con índice $33$ antes y después de la ecualización de histograma}
	\label{fig:face33}
\end{figure}

\begin{itemize}
	\item \textbf{Normalización:} aplicación de \lstinline{StandardScaler} de \lstinline{scikit-learn}~\cite{pedregosa2011sklearn} para centrar y escalar cada píxel (ver Figura~\ref{fig:scaled42}). Con esto, el PCA no se ve dominado por píxeles con varianza naturalmente más alta.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/scaled42.png}
	\caption{Imagen con índice $42$ después de normalización con StandardScaler}
	\label{fig:scaled42}
\end{figure}

\begin{itemize}
	\item \textbf{Reducción de dimensionalidad:} PCA con 201 componentes principales, conservando el 90\% de la varianza (reducción de 2,304 a 201 dimensiones, 8.7\% del original). Se fijó \lstinline{random_state=42} para reproducibilidad. La Figura~\ref{fig:explained_variance} muestra la varianza explicada acumulada, y la Figura~\ref{fig:beforeAfterPCA} muestra la reconstrucción de una imagen tras aplicar PCA.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.48\textwidth]{images/explainedVariance.png}
	\caption{Varianza explicada acumulada vs número de componentes PCA. La línea verde indica el umbral del 90\% alcanzado con 201 componentes.}
	\label{fig:explained_variance}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/beforeAfterPCA.png}
	\caption{Reconstrucción de imagen antes y después de aplicar PCA con 201 componentes (90\% varianza).}
	\label{fig:beforeAfterPCA}
\end{figure}

\subsection{Algoritmos}

Ambos algoritmos se ejecutaron con las implementaciones de \lstinline{scikit-learn}~\cite{pedregosa2011sklearn} y \lstinline{random_state=42} para que las diferencias entre corridas sean comparables. Los valores de $k$ explorados satisfacen la instrucción del curso ($1 < k < 20$) y cubren la hipótesis de que podrían existir entre dos y varias emociones base en el conjunto de datos.

\textbf{K-Means:} algoritmo basado en particiones que agrupa los datos según la distancia a centroides. Funciona iterativamente:

\begin{enumerate}
	\item Asigna cada punto al centroide más cercano.
	\item Recalcula los centroides como el promedio de los puntos asignados.
\end{enumerate}

Este proceso se repite hasta la convergencia. Supone clústeres esféricos y de tamaño similar.

\textbf{Gaussian Mixture Model (GMM):} modelo probabilístico que asume que los datos provienen de una mezcla de distribuciones gaussianas. Cada punto tiene una probabilidad de pertenecer a cada clúster. Ajuste mediante el algoritmo EM (Expectation-Maximization)~\cite{dempster1977em}.

Las Tablas~\ref{tab:params_kmeans} y~\ref{tab:params_gmm} muestran los parámetros utilizados para cada algoritmo.

\begin{table}[H]
	\centering
	\caption{Parámetros de K-Means}
	\label{tab:params_kmeans}
	\footnotesize
	\begin{tabular}{|l|l|p{2.8cm}|}
		\hline
		\textbf{Parámetro}    & \textbf{Valor}        & \textbf{Descripción} \\
		\hline
		\lstinline|init|      & \lstinline|k-means++| & Init. estable        \\
		\lstinline|n_init|    & \lstinline|auto|      & Múltiples init.      \\
		\lstinline|max_iter|  & \lstinline|500|       & Límite iter.         \\
		\lstinline|algorithm| & \lstinline|elkan|     & Optim. eficiente     \\
		\hline
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption{Parámetros de GMM}
	\label{tab:params_gmm}
	\footnotesize
	\begin{tabular}{|l|l|p{2.8cm}|}
		\hline
		\textbf{Parámetro}      & \textbf{Valor}        & \textbf{Descripción} \\
		\hline
		\lstinline|covar_type|  & \lstinline|spherical| & Simplifica modelo    \\
		\lstinline|n_init|      & \lstinline|5|         & Múltiples init.      \\
		\lstinline|init_params| & \lstinline|kmeans|    & Init. de medias      \\
		\lstinline|max_iter|    & \lstinline|500|       & Límite iter.         \\
		\lstinline|tol|         & \lstinline|1e-3|      & Tolerancia conv.     \\
		\lstinline|reg_covar|   & \lstinline|1e-6|      & Regularización       \\
		\hline
	\end{tabular}
\end{table}

\subsection{Métricas de Evaluación}

\begin{itemize}
	\item \textbf{Coeficiente Silhouette:} mide la cohesión y la separación de los clústeres. Valores cercanos a 1 indican buena separación; valores cercanos a 0 indican solapamiento; valores negativos indican mala asignación. Se calculó con una muestra de hasta 10,000 puntos (\lstinline{random_state=42}) para reducir coste computacional manteniendo representatividad.
	\item \textbf{Índice Davies-Bouldin (DBI):} evalúa la relación entre dispersión intra-clúster y separación inter-clúster. Valores bajos indican clústeres compactos y bien separados.
\end{itemize}

\subsection{Diseño Experimental}

Se evaluaron valores de $k$ en el rango $[2, 20]$. Para K-Means se utilizó la inercia (WCSS) como criterio del codo, y para GMM se emplearon BIC y AIC. Las métricas Silhouette y DBI se calcularon para cada partición y se registraron en tablas.

Todo el flujo se ejecutó en Python, empleando NumPy, pandas, OpenCV y \lstinline{scikit-learn}~\cite{pedregosa2011sklearn}. El \textit{Jupyter Notebook} junto a este informe permite reproducir los comandos y gráficas mostradas aquí.

\section{Resultados}

\subsection{Métricas Internas}

La Tabla~\ref{tab:metricas} muestra los valores óptimos de Silhouette y DBI obtenidos para cada algoritmo.

\begin{table}[htbp]
	\centering
	\caption{Comparación de métricas en valores óptimos}
	\label{tab:metricas}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Algoritmo} & \textbf{k óptimo} & \textbf{Silhouette} & \textbf{DBI} \\
		\hline
		K-Means            & 2                 & 0.094               & 3.05         \\
		GMM                & 3                 & 0.083               & 2.62         \\
		\hline
	\end{tabular}
\end{table}

En ambos casos, el rango completo de $k$ produjo coeficientes Silhouette entre 0.05 y 0.10, mientras que el índice DBI permaneció por encima de 2.5. Estos números confirman que la estructura latente es débil y que los clústeres resultan muy solapados aun después de reducir la dimensionalidad.

\subsection{Criterio del Codo}

Las Figuras~\ref{fig:kmeans_codo} y~\ref{fig:gmm_codo} muestran las curvas de inercia (K-Means) y BIC/AIC (GMM).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/codoKMeans.png}
	\caption{Criterio del codo para K-Means (Inercia)}
	\label{fig:kmeans_codo}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/codoGMM.png}
	\caption{Criterio del codo para GMM (BIC/AIC)}
	\label{fig:gmm_codo}
\end{figure}

\section{Análisis de Resultados}

Los valores de Silhouette son bajos ($<0.1$), lo que indica una separación débil entre clústeres. El índice DBI mejora al pasar de $k=2$ a $k=3$.

\begin{itemize}
	\item \textbf{K-Means:} $k=2$ maximiza Silhouette (0.094); $k=3$ mejora DBI (3.05 $\rightarrow$ 2.62).
	\item \textbf{GMM:} $k=3$ es óptimo por DBI (2.62), con Silhouette similar a K-Means (0.083).
\end{itemize}

La inercia de K-Means disminuye rápidamente hasta $k \approx 3$, sin mostrar un codo marcado. En GMM, BIC y AIC descienden casi linealmente, sin punto de inflexión claro. Ambos análisis indican que $k=3$ balancea simplicidad y calidad de agrupamiento.

\section{Conclusiones}

Los datos de expresiones faciales no presentan una estructura de clústeres bien definida. Las métricas Silhouette ($<0.1$) y Davies-Bouldin ($>2.6$) indican solapamiento significativo entre grupos.

PCA redujo la dimensionalidad de 2304 a 201 características, manteniendo el 90\% de la varianza, pero no mejoró la separabilidad. K-Means y GMM convergen a soluciones similares con $k=2$ o $k=3$ clústeres. En ambos casos, el criterio del codo no muestra punto de inflexión claro.

Los resultados sugieren que la variabilidad en las expresiones faciales no se captura adecuadamente mediante agrupamiento esférico basado en la distancia euclidiana.

\section{Trabajo Futuro}

Para mejorar la calidad del agrupamiento se propone:

\begin{itemize}
	\item Evaluar algoritmos basados en densidad (DBSCAN~\cite{ester1996dbscan}, HDBSCAN~\cite{campello2013hdbscan}) que no asumen una forma esférica.
	\item Probar métricas alternativas como Calinski-Harabasz o Índice de Dunn.
	\item Explorar técnicas de reducción de dimensionalidad no lineales (t-SNE~\cite{maaten2008tsne}, UMAP~\cite{mcinnes2018umap}) previas al agrupamiento.
	\item Utilizar \textit{autoencoders}~\cite{hinton2006reducing} para realizar una reducción de dimensionalidad más discriminativa que PCA.
\end{itemize}

\section*{Nota Aclarativa}

Se han utilizado las siguientes herramientas de IA generativa: ChatGPT (GPT-5, GPT-5-Codex), GitHub Copilot (GPT-5, Claude Sonnet 4.5). Su uso se limitó a la revisión de código y redacción, la ayuda para el análisis de gráficos y resultados, la consulta de documentación de librerías y la búsqueda de información para incrementar nuestro conocimiento. Todo el contenido ha sido revisado, verificado y complementado con nuestro aporte personal.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
