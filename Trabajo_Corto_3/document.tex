\documentclass[journal]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{listings}
\usepackage{float}

\lstset{basicstyle=\ttfamily\footnotesize,columns=flexible,breaklines=true}

\title{Trabajo Práctico 3}

\author{
  \IEEEauthorblockN{Kenneth Eduardo Barboza Corrales\IEEEauthorrefmark{1} \and David Alberto Guevara Sánchez\IEEEauthorrefmark{1}}\\
  \IEEEauthorblockA{\IEEEauthorrefmark{1}Maestría en Ciencias de la Computación, Tecnológico de Costa Rica}
}

\begin{document}

\maketitle

\begin{abstract}
	Este trabajo presenta un experimento de agrupamiento no supervisado sobre un conjunto de 33,693 imágenes en escala de grises (48x48 píxeles) que representan emociones humanas. Se aplicó preprocesamiento mediante ecualización de histograma, normalización y reducción de dimensionalidad con PCA (201 componentes, 90\% varianza explicada). Se evaluaron dos algoritmos: K-Means y Gaussian Mixture Model (GMM), utilizando métricas internas Silhouette y Davies-Bouldin. Los resultados muestran baja separación entre clústeres, con valores óptimos en rangos reducidos de $k$.
\end{abstract}

\begin{IEEEkeywords}
	Aprendizaje no supervisado, Clustering, K-Means, Gaussian Mixture Model, PCA, Silhouette, Davies-Bouldin
\end{IEEEkeywords}

\section{Introducción}

Este trabajo aplica técnicas de agrupamiento no supervisado para descubrir patrones en un conjunto de imágenes faciales en escala de grises. El objetivo es evaluar si los datos presentan estructura coherente con categorías emocionales, sin disponer de etiquetas previas.

El preprocesamiento incluyó ecualización de histograma, normalización y reducción de dimensionalidad mediante PCA~\cite{jolliffe2002pca}, reteniendo el 90\% de la varianza. Se implementaron dos algoritmos: K-Means~\cite{macqueen1967kmeans} y Gaussian Mixture Models (GMM)~\cite{dempster1977em}.

La calidad del agrupamiento se evaluó en función del número de clústeres ($k$) mediante métricas internas (Silhouette~\cite{rousseeuw1987silhouette} y Davies-Bouldin~\cite{davies1979dbi}) y el criterio del codo.

\section{Trabajos relacionados - Estado del arte}

El agrupamiento de imágenes faciales es un problema estudiado en diversos contextos. K-Means~\cite{macqueen1967kmeans} y GMM~\cite{dempster1977em} son métodos comunes de aprendizaje no supervisado para descubrir patrones en datos sin etiquetas.

PCA~\cite{jolliffe2002pca} es una técnica estándar para reducir dimensionalidad en imágenes de alta dimensionalidad. Reduce el coste computacional mientras preserva la mayor parte de la información relevante.

En reconocimiento facial y análisis de expresiones, el agrupamiento no supervisado enfrenta desafíos debido a la alta variabilidad intra-clase y similitud inter-clase. Métricas como Silhouette~\cite{rousseeuw1987silhouette} y Davies-Bouldin~\cite{davies1979dbi} permiten evaluar calidad de agrupamiento sin etiquetas verdaderas.

\section{Conceptos clave (Background)}

\begin{itemize}
	\item \textbf{Aprendizaje No Supervisado:} búsqueda de patrones en datos sin etiquetas previas de clase.
	\item \textbf{Reducción de Dimensionalidad (PCA):} el \textit{Análisis de Componentes Principales (PCA)}~\cite{jolliffe2002pca} transforma datos originales en un espacio reducido preservando la mayor varianza posible. En este trabajo se utilizaron 201 componentes (reducción al 8.7\% con respecto al original), reteniendo el 90\% de la varianza.
	\item \textbf{Algoritmo K-Means:}~\cite{macqueen1967kmeans} algoritmo de partición que agrupa datos según proximidad a centroides. Asume clústeres esféricos y de tamaño similar.
	\item \textbf{Modelo de Mezcla Gaussiana (GMM):}~\cite{dempster1977em} modelo probabilístico que asume que los datos provienen de una mezcla de distribuciones gaussianas. Cada punto tiene una probabilidad de pertenecer a cada clúster.
	\item \textbf{Métricas Internas:}
	      \begin{itemize}
		      \item \textit{Silhouette}~\cite{rousseeuw1987silhouette}: mide cohesión y separación de clústeres. Valores en $[-1, 1]$; mayor es mejor.
		      \item \textit{Índice Davies-Bouldin (DBI)}~\cite{davies1979dbi}: evalúa la relación entre dispersión intra-clúster y separación inter-clúster. Menor es mejor.
	      \end{itemize}
\end{itemize}

\section{Materiales y métodos}

\subsection{Conjunto de Datos}

Se utilizó el dataset \lstinline{datos_imagenes_USAR.csv}, originalmente compuesto por 35,887 imágenes en escala de grises de 48x48 píxeles (ver Figura~\ref{fig:face33_raw}).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/face33.png}
	\caption{Imagen con el índice $33$ del \textit{dataset} antes del preprocesamiento}
	\label{fig:face33_raw}
\end{figure}

Antes de preprocesar el conjunto se procedió a:

\begin{itemize}
	\item Eliminar 1,853 filas duplicadas.
	\item Descartar 341 imágenes con menor varianza (percentil inferior 1\%). Las Figuras~\ref{fig:worst10} y~\ref{fig:best10} muestran ejemplos de imágenes con muy baja y alta varianza.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/worst10.png}
	\caption{Imágenes con muy poca varianza}
	\label{fig:worst10}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/best10.png}
	\caption{Imágenes con mayor varianza}
	\label{fig:best10}
\end{figure}

Después del filtrado, se trabajó con 33,693 imágenes (93.9\% del conjunto original).

\subsection{Preprocesamiento}

\begin{itemize}
	\item \textbf{Ecualización de histograma:} mejora el contraste en las imágenes (ver Figura~\ref{fig:face33}).
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/beforeAfterEqui.png}
	\caption{Comparación de la imagen con índice $33$ antes y después de la ecualización de histograma}
	\label{fig:face33}
\end{figure}

\begin{itemize}
	\item \textbf{Normalización:} aplicación de \lstinline{StandardScaler} para escalar características (ver Figura~\ref{fig:scaled42}).
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/scaled42.png}
	\caption{Imagen con índice $42$ después de normalización con StandardScaler}
	\label{fig:scaled42}
\end{figure}

\begin{itemize}
	\item \textbf{Reducción de dimensionalidad:} PCA con 201 componentes, conservando el 90\% de la varianza (reducción de 2304 a 201 dimensiones, 8.7\% del original). La Figura~\ref{fig:explained_variance} muestra la varianza explicada acumulada, y la Figura~\ref{fig:beforeAfterPCA} muestra la reconstrucción de una imagen tras aplicar PCA.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.48\textwidth]{images/explainedVariance.png}
	\caption{Varianza explicada acumulada vs número de componentes PCA. La línea verde indica el umbral del 90\% alcanzado con 201 componentes.}
	\label{fig:explained_variance}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/beforeAfterPCA.png}
	\caption{Reconstrucción de imagen antes y después de aplicar PCA con 201 componentes (90\% varianza).}
	\label{fig:beforeAfterPCA}
\end{figure}

\subsection{Algoritmos}

\textbf{K-Means:} algoritmo basado en partición que agrupa los datos según la distancia a centroides. Funciona iterativamente:

\begin{enumerate}
	\item Asigna cada punto al centroide más cercano.
	\item Recalcula los centroides como el promedio de los puntos asignados.
\end{enumerate}

Este proceso se repite hasta la convergencia. Supone clústeres esféricos y de tamaño similar.

\textbf{Gaussian Mixture Model (GMM):} modelo probabilístico que asume que los datos provienen de una mezcla de distribuciones gaussianas. Cada punto tiene una probabilidad de pertenecer a cada clúster. Ajuste mediante algoritmo EM (Expectation-Maximization)~\cite{dempster1977em}.

Las Tablas~\ref{tab:params_kmeans} y~\ref{tab:params_gmm} muestran los parámetros utilizados para cada algoritmo.

\begin{table}[H]
	\centering
	\caption{Parámetros de K-Means}
	\label{tab:params_kmeans}
	\footnotesize
	\begin{tabular}{|l|l|p{2.8cm}|}
		\hline
		\textbf{Parámetro}    & \textbf{Valor}        & \textbf{Descripción} \\
		\hline
		\lstinline|init|      & \lstinline|k-means++| & Init. estable        \\
		\lstinline|n_init|    & \lstinline|auto|      & Múltiples init.      \\
		\lstinline|max_iter|  & \lstinline|500|       & Límite iter.         \\
		\lstinline|algorithm| & \lstinline|elkan|     & Optim. eficiente     \\
		\hline
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption{Parámetros de GMM}
	\label{tab:params_gmm}
	\footnotesize
	\begin{tabular}{|l|l|p{2.8cm}|}
		\hline
		\textbf{Parámetro}      & \textbf{Valor}        & \textbf{Descripción} \\
		\hline
		\lstinline|covar_type|  & \lstinline|spherical| & Simplifica modelo    \\
		\lstinline|n_init|      & \lstinline|5|         & Múltiples init.      \\
		\lstinline|init_params| & \lstinline|kmeans|    & Init. de medias      \\
		\lstinline|max_iter|    & \lstinline|500|       & Límite iter.         \\
		\lstinline|tol|         & \lstinline|1e-3|      & Tolerancia conv.     \\
		\lstinline|reg_covar|   & \lstinline|1e-6|      & Regularización       \\
		\hline
	\end{tabular}
\end{table}

\subsection{Métricas de Evaluación}

\begin{itemize}
	\item \textbf{Coeficiente Silhouette:} mide cohesión y separación de clústeres. Valores cercanos a 1 indican buena separación; valores cercanos a 0 indican solapamiento; valores negativos indican mala asignación. Se calculó con una muestra de 10,000 puntos para reducir coste computacional.
	\item \textbf{Índice Davies-Bouldin (DBI):} evalúa la relación entre dispersión intra-clúster y separación inter-clúster. Valores bajos indican clústeres compactos y bien separados.
\end{itemize}

\subsection{Diseño Experimental}

Se evaluaron valores de $k$ en el rango $[2, 20]$. Para K-Means se utilizó la inercia (WCSS) como criterio del codo, y para GMM se emplearon BIC y AIC. Las métricas Silhouette y DBI se calcularon para cada partición.

\section{Resultados}

\subsection{Métricas Internas}

La Tabla~\ref{tab:metricas} muestra los valores óptimos de Silhouette y DBI obtenidos para cada algoritmo.

\begin{table}[htbp]
	\centering
	\caption{Comparación de métricas en valores óptimos}
	\label{tab:metricas}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Algoritmo} & \textbf{k óptimo} & \textbf{Silhouette} & \textbf{DBI} \\
		\hline
		K-Means            & 2                 & 0.094               & 3.05         \\
		GMM                & 3                 & 0.083               & 2.62         \\
		\hline
	\end{tabular}
\end{table}

\subsection{Criterio del Codo}

Las Figuras~\ref{fig:kmeans_codo} y~\ref{fig:gmm_codo} muestran las curvas de inercia (K-Means) y BIC/AIC (GMM).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/codoKMeans.png}
	\caption{Criterio del codo para K-Means (Inercia)}
	\label{fig:kmeans_codo}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{images/codoGMM.png}
	\caption{Criterio del codo para GMM (BIC/AIC)}
	\label{fig:gmm_codo}
\end{figure}

\section{Análisis de Resultados}

Los valores de Silhouette son bajos ($<0.1$), indicando separación débil entre clústeres. El índice DBI mejora al pasar de $k=2$ a $k=3$.

\begin{itemize}
	\item \textbf{K-Means:} $k=2$ maximiza Silhouette (0.094); $k=3$ mejora DBI (3.05 $\rightarrow$ 2.62).
	\item \textbf{GMM:} $k=3$ es óptimo por DBI (2.62), con Silhouette similar a K-Means (0.083).
\end{itemize}

La inercia de K-Means disminuye rápidamente hasta $k \approx 3$, sin mostrar un codo marcado. En GMM, BIC y AIC descienden casi linealmente, sin punto de inflexión claro. Ambos análisis indican que $k=3$ balancea simplicidad y calidad de agrupamiento.

\section{Conclusiones}

Los datos de expresiones faciales no presentan una estructura de clúster bien definida. Las métricas Silhouette ($<0.1$) y Davies-Bouldin ($>2.6$) indican solapamiento significativo entre grupos.

PCA redujo dimensionalidad de 2304 a 201 características manteniendo 90\% de varianza, pero no mejoró la separabilidad. K-Means y GMM convergen a soluciones similares con $k=2$ o $k=3$ clústeres. En ambos casos, el criterio del codo no muestra punto de inflexión claro.

Los resultados sugieren que la variabilidad en expresiones faciales no se captura adecuadamente mediante agrupamiento esférico basado en distancia euclidiana.

\section{Trabajo Futuro}

Para mejorar la calidad del agrupamiento se propone:

\begin{itemize}
	\item Evaluar algoritmos basados en densidad (DBSCAN, HDBSCAN) que no asumen una forma esférica.
	\item Probar métricas alternativas como Calinski-Harabasz o Índice de Dunn.
	\item Explorar técnicas de reducción de dimensionalidad no lineales (t-SNE, UMAP) antes del agrupamiento.
	\item Utilizar \textit{autoencoders} para realizar una reducción de dimensionalidad más discriminativa que PCA.
\end{itemize}

\section*{Nota Aclarativa}

Se han utilizado las siguientes herramientas de IA generativa: ChatGPT (GPT-5, GPT-5-Codex), GitHub Copilot (GPT-5, Claude Sonnet 4.5). Su uso se limitó a revisión de código y redacción, ayuda para el análisis de gráficos y resultados, consulta de documentación de librerías, y búsqueda de información para incrementar nuestro conocimiento. Todo el contenido ha sido revisado, verificado y complementado con nuestro aporte personal.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
